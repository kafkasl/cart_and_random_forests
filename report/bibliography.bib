@book{Quinlan1993,
abstract = {This book is a complete guide to the C4.5 system as implemented in C for the UNIX environment. It contains a comprehensive guide to the system's use, the source code (about 8,800 lines), and implementation notes. Front Cover; C4.5: Programs for Machine Learning; Copyright Page; Table of Contents; Preface; Obtaining the C4.5 Code; CHAPTER 1. Introduction; 1.1 Example: Labor negotiation settlements; 1.2 Other kinds of classification models; 1.3 What lies ahead; CHAPTER 2. Constructing Decision Trees; 2.1 Divide and conquer; 2.2 Evaluating tests; 2.3 Possible tests considered; 2.4 Tests on continuous attributes; CHAPTER 3. Unknown Attribute Values; 3.1 Adapting the previous algorithms; 3.2 Play/Don't Play example again; 3.3 Recapitulation; CHAPTER 4. Pruning Decision Trees; 4.1 When to simplify? 4.2 Error-based pruning4.3 Example: Democrats and Republicans; 4.4 Estimating error rates for trees; CHAPTER 5. From Trees to Rules; 5.1 Generalizing single rules; 5.2 Class rulesets; 5.3 Ranking classes and choosing a default; 5.4 Summary; CHAPTER 6. Windowing; 6.1 Example: Hypothyroid conditions revisited; 6.2 Why retain windowing?; 6.3 Example: The multiplexor; CHAPTER 7. Grouping Attribute Values; 7.1 Finding value groups by merging; 7.2 Example: Soybean diseases; 7.3 When to form groups?; 7.4 Example: The Monk's problems; 7.5 Uneasy reflections. CHAPTER 8. Interacting with Classification Models8.1 Decision tree models; 8.2 Production rule models; 8.3 Caveat; CHAPTER 9. Guide to Using the System; 9.1 Files; 9.2 Running the programs; 9.3 Conducting experiments; 9.4 Using options: A credit approval example; CHAPTER 10. Limitations; 10.1 Geometric interpretation; 10.2 Nonrectangular regions; 10.3 Poorly delineated regions; 10.4 Fragmented regions; 10.5 A more cheerful note; CHAPTER 11. Desirable Additions; 11.1 Continuous classes; 11.2 Ordered discrete attributes; 11.3 Structured attributes; 11.4 Structured induction. 11.5 Incremental induction11.6 Prospectus; Appendix: Program Listings; Brief descriptions of the contents of the files; Notes on some important data structures; File Makefile; Alphabetic index of routines; References and Bibliography; Author Index; Subject Index.},
author = {Quinlan, J. R. (John Ross)},
isbn = {9780080500584},
pages = {302},
publisher = {Morgan Kaufmann Publishers},
title = {{C4.5 : programs for machine learning}},
url = {https://books.google.es/books?hl=en{\&}lr={\&}id=b3ujBQAAQBAJ{\&}oi=fnd{\&}pg=PP1{\&}dq=c4.5{\&}ots=sPapTKElE3{\&}sig=10Azg9zloUUSg438O1Hfw253bH0{\#}v=onepage{\&}q=c4.5{\&}f=false},
year = {1993}
}
@article{Dietterich2000,
author = {Dietterich, Thomas G.},
doi = {10.1023/A:1007607513941},
file = {:home/hydra/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dietterich - 2000 - An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees Bagging, Boosting, and Rand.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
number = {2},
pages = {139--157},
publisher = {Kluwer Academic Publishers},
title = {{An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees: Bagging, Boosting, and Randomization}},
url = {http://link.springer.com/10.1023/A:1007607513941},
volume = {40},
year = {2000}
}
@article{Breiman2001,
abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the corre-lation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\&} R. Schapire, Machine Learning: Proceedings of the Thirteenth Interna-tional conference, * * * , 148â€“156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
author = {Breiman, Leo},
file = {:home/hydra/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Breiman - 2001 - Random Forests.pdf:pdf},
journal = {Machine Learning},
keywords = {classification,ensemble,regression},
pages = {5--32},
title = {{Random Forests}},
url = {http://download.springer.com.recursos.biblioteca.upc.edu/static/pdf/639/art{\%}253A10.1023{\%}252FA{\%}253A1010933404324.pdf?originUrl=http{\%}3A{\%}2F{\%}2Flink.springer.com{\%}2Farticle{\%}2F10.1023{\%}2FA{\%}3A1010933404324{\&}token2=exp=1493022481{~}acl={\%}2Fstatic{\%}2Fpdf{\%}2F639{\%}2Fart{\%}2525},
volume = {45},
year = {2001}
}
